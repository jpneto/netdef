<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta http-equiv="Content-Language" content="en-us">
<title>PhD work</title>
</head>

<body background="glabkgnd.jpg" bgcolor="#FFFFFF" text="#000000" link="#006666" vlink="#999999" alink="#66CCCC">

<p align="center"><b><font size="+2">João Pedro Neto's<br>PhD work</font></b></p>

<ul>
	<li><strong>Abstract</strong>

<p><font size="3">

This work lies within the scientific areas of Theory of Computation and artificial neural
networks. It researches some possible knowledge bridges between
both areas, and tries to integrate concepts in order to achieve a common and broader computational framework.&nbsp;</font></p>

<p><font size="3">

 This effort concentrates, firstly, on a computational architecture definition, based on a certain neural model (and subsequent demonstration that its power is equivalent to Turing Machines). A working compiler that translates the demonstration of
these computational paradigms is presented <a href="nwb.html">here</a>. There is
also a tool called <a href="zips/netway.zip">NETWAY</a> which is able to
simulate this type of neural networks with arbitrary precision.</font></p>

<p><font size="3">

 Secondly, it was developed a set of working
tools to maximize and take advantage of this computational model. This was achieved by using a high level programming language, and an automatic
compilation process, able to translate the algorithmic representation of a certain problem into a parallel and modular neural network. This language is
called NETDEF and there is a compiler <a href="netdef.html">available</a>. These tools focus on two computational
concepts: control and learning. In this context, control means all algorithms that use symbolic information, i.e., information
with a well-defined context and meaning. Learning, on the other side, consists in a set of sub-symbolic algorithms, where there is no individual 
meaning for each basic piece of information, and all knowledge is distributed.</font>&nbsp;</p>
	<p><b>New:</b> There is a Java program (a <i>runnable</i> jar) that 
	translates neural nets of this model into a set of triples that can be 
	executed in parallel. This translator plus a simulator for this triple 
	processing is
	<a href="http://homepages.di.fc.ul.pt/~jpn/netdef/zips/nn_simulator.zip">
	available</a>.</p>
      <p><i>Note</i>: Unless stated otherwise, all software tools are for <font size="3">Windows™ only.</font></p>
      </li>
</ul>
<ul>
	<li><strong> Publications</strong>
      <p>J. Neto, J. F. Costa, P. Carreira, and M. Rosa. <i>A compiler and
      simulator for partial recursive functions over neural networks</i>, in
      Ahmad Lotfi and Jonathon M. Garibaldi (eds), <b>Applications and Science
      in Soft Computing</b>, Springer-Verlag, 2004, in print. <font size="3">
      Download it in <a href="papers/rasc2002.pdf">pdf</a> format.</font> </p>
      <p><font size="3">
      J. Neto, H. Siegelmann, J. Costa. <i>Symbolic Processing in Neural Networks</i>, <b>Journal of Brazilian Computer Science</b>,
      8(3), 58-70, 2003. Download it in <a href="papers/jbcs_ps.gz">gziped
      postscript</a> or <a href="papers/jbcs.pdf">pdf</a> format.</font> </p>
      <p><font size="3"><span lang="EN-US" style="font-size:12.0pt;
mso-bidi-font-size:10.0pt;mso-ansi-language:EN-US">J. Neto and H. Coelho. <i>A
      distributed search (best-path) algorithm based on local communication</i>,
      <b>International Conference on Intelligent Agents, Web Technologies and
      Internet Commerce</b> (IAWTIC'2003), 377-384, 2003.<o:p>
      Download in <a href="papers/artMCMO-cimca2.pdf">pdf</a> format.</o:p></span></font></p>
      <p><font size="3">J. Neto, H. Coelho, A. Ferreira. <i>Competitive Learning
      and Symbolic Computation Integration</i>, <b>5th Brazilian Congress of
      Neural Nets</b>, 19-24, 2001.</font> </p>
      <p><font size="3">
      J. Neto, J. Costa, A. Ferreira. <i>Merging Sub-symbolic and Symbolic Computation</i>, 
      <b>Proceedings of the Second International ICSC Symposium on Neural Computation NC'00</b> (Bothe, H.
      and Rojas, R., Eds.), ICSC Academic Press, (2000), 329-335..</font> Download it in <a
      href="papers/nc2k_ps.gz">gziped postscript</a> or <a href="papers/nc2000.pdf">pdf</a> format.</p>
      <p>J. Neto, H. Siegelmann, J. Costa, <i>Information Coding
      and Neural Computing,</i> <b>Advances in A.I. and Engineering Cybernetics,
      Vol. IV: Systems Logic &amp; Neural Networks</b>, IIAS, [10], 1998, 76-80. Download it on <a href="papers/intersys.pdf">pdf</a> format.</p>
      <p><font size="3">
      J. Neto, H. Siegelmann, J. Costa.
      <i>On the Implementation of Programming Languages with Neural Nets</i>. In <b>First
      International Conference on Computing Anticipatory Systems, CASYS 97</b>, [1], 201-208,
      CHAOS, 1998. Download it on <a href="papers/casys97.pdf">pdf</a> format.</font> </p>
      <p><font size="3">
      J. Neto, H. Siegelmann, J. Costa, C. Araújo. <i>Turing
      Universality of Neural Nets (revisited)</i>. <b>
		<a href="http://dx.doi.org/10.1007/BFb0025058">Lecture Notes in Computer Science &#150; 1333</a></b>
      , 361-366, Springer-Verlag, 1997. Download it in <a
      href="papers/ecast_ps.gz">gziped postscript</a> or <a href="papers/eurocast.pdf">pdf</a> format.</font> </p>
      <p>J. Neto, J. Costa, H. Coelho, <i>Lower Bounds of a Computacional Power of a Synaptic Calculus</i>. In <b>
		<a href="http://dx.doi.org/10.1007/BFb0032492">Lecture Notes in
      Computer Science - 1240</a></b>, 340-348, Springer-Verlag, 1997. 
      Download it in <a href="papers/iwann_ps.gz">gziped postscript</a> or <a href="papers/iwann.pdf">pdf</a> format. </p>
      <p>The <a href="zips/tese.zip">thesis</a> is also available in pdf (however, it
      is in Portuguese).</p>
	</li>
	<li><b>Some open questions</b>
      <p>The NETDEF neural networks are synchronous, i.e., they need a global
      clock to update at the same time. Is it possible to devise an architecture
      able to support asynchronous update? That would mean that each neuron
      would process in its own time, approaching the apparent dynamics of the
      neural central system of real animals.</p>
      <p>Non homogenous activation functions seems another direction to expand
      NETDEF. The saturated sigmoid used is appropriate for symbolic computation
      and some learning functions are implementable (like Hebb or Competitive
      Learning). Others, like Backpropagation are not (at least in trivial
      ways). The insertion of other activation functions (like the sigmoid) can
      provide expedite ways to enlarge the scope of available learning
      functions.</p>
      <p>Including dynamical networks, i.e., creation and destruction of neurons
      over time, can be the major step to include both recursion over symbolic
      functions, and the use of learning functions that use that concept (like
      ART networks)</p>
      <p>Improving the integration of symbolic and subsymbolic computation. This
      integration, made upon NETDEF, where is possible
      to define complex symbolic tasks and assign them to neural nets, this next step
      created interaction between this unusual way to use neural nets with the more standard
      methods of neural nets, namely, learning and classification. There are two main goals: (a) interface definition, i.e., how symbolic and subsymbolic modules can
      interact and communicate, and (b) how learning may be conducted in neural nets using only
      neural nets, i.e., instead of thinking of a neural net as a special data structure in a
      von Neumann computer. The goal was to find a neural net description (a set of discrete
      dynamic equations) that can learn but also is able to control the learning process.
      Future steps may include:</p>
      <ul>
		<li>Implement
            other learning laws, besides Hebb's Law and Competitive Learning.</li>
		<li>Try to define learning modules as general as possible. (Felix asked if some kind of
          &nbsp; 'universal learning module' makes sense. Does it?).</li>
		<li>Define ways to preprocess sample information, like filtering sample features.</li>
		<li>How to embed predefined knowledge in learning modules.</li>
	</ul>
	</li>
</ul>

<p ALIGN="right"><i><font size="2">May 2002</font></i></p>
</body>

</html>
